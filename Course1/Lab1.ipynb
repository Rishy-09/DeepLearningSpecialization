{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edcd7a1",
   "metadata": {},
   "source": [
    "## Python basics with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf4c34",
   "metadata": {},
   "source": [
    "### 1. Building basic functions with numpy\n",
    "\n",
    "#### 1.1 - sigmoid function, np.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612713ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c79f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820137900379085"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30142a76",
   "metadata": {},
   "source": [
    "as input of this was a real number we barely use math library in deep learning as we mostly use matrices and vectors so numpy is prefered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6687b61f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m))\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acacbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23ae6e",
   "metadata": {},
   "source": [
    "implementing sigmoid using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635638e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693d1a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(3)\n",
    "sigmoid(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2982d57",
   "metadata": {},
   "source": [
    "#### 1.2 Sigmoid gradient:\n",
    "\n",
    "as we need to compute gradients to optimize loss functions using backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7de2ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 -s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0438bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b2b941c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.10499359, 0.04517666])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde73e4",
   "metadata": {},
   "source": [
    "#### 1.3 Reshaping arrays:\n",
    "Two common numpy functions that are used in deep learning are np.shape() and np.reshape().\n",
    "\n",
    "x.shape is used to get the shape of matrix/vector x.\n",
    "x.reshape(...) is used to reshape x into some other dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ccf38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecaecb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67826139],\n",
       "       [0.29380381],\n",
       "       [0.90714982],\n",
       "       [0.52835647],\n",
       "       [0.4215251 ],\n",
       "       [0.45017551],\n",
       "       [0.92814219],\n",
       "       [0.96677647],\n",
       "       [0.85304703],\n",
       "       [0.52351845],\n",
       "       [0.19981397],\n",
       "       [0.27417313],\n",
       "       [0.60659855],\n",
       "       [0.00533165],\n",
       "       [0.10820313],\n",
       "       [0.49978937],\n",
       "       [0.34144279],\n",
       "       [0.94630077]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "image2vector(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb6449",
   "metadata": {},
   "source": [
    "#### 1.4 Normalizing rows:\n",
    "\n",
    "In ML and DL we normalize our data. it often leads to a better performance as gradient converges faster after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9fd0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    # normalization is along the rows. (to have a unit length)\n",
    "    # np.linalg.norm: NumPy Linear Algebra Norm\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    print(x_norm.shape)\n",
    "    x = x / x_norm\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51945e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e0071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeRows(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ed846",
   "metadata": {},
   "source": [
    "here as the shape of x_norm is (2, 1) i.e. single value in whole row in each row (as expected) so broadcasting come into role when it is being divided by x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d6c618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x) # for each row\n",
    "    # now store sum of each exp of x values\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    return x_exp / x_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd5034d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04,\n",
       "        1.21052389e-04],\n",
       "       [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04,\n",
       "        8.01252314e-04]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc12a8",
   "metadata": {},
   "source": [
    "### 2. Vectorization:\n",
    "dot/outer/elementwise.\n",
    "Dot product (inner product): collapses two vectors into a single number.\n",
    "Example:\n",
    "\n",
    "[a,b]⋅[c,d]=ac+bd\n",
    "\n",
    "Outer product: expands two vectors into a full matrix of pairwise products.\n",
    "Example:\n",
    "\n",
    "[a,b]⊗[c,d]=[a⋅c    b⋅c\n",
    "\t          a⋅d     b⋅d]\n",
    "              \n",
    "Outer product shows up in linear algebra (rank-1 matrices).\n",
    "\n",
    "In ML, it’s used in covariance matrices, embeddings, attention mechanisms (query ⊗ key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00720dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1c6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken0.17240000306628644ms.\n",
      "Time taken0.36860001273453236ms.\n",
      "Time taken0.14459999511018395ms.\n",
      "Time taken0.23170001804828644ms.\n"
     ]
    }
   ],
   "source": [
    "# classic dot product\n",
    "tic = time.perf_counter()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n",
    "# output product implementation\n",
    "tic = time.perf_counter()\n",
    "outer = np.zeros((len(x1), len(x2)))\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i, j] = x1[i]*x2[j]\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n",
    "# elment-wise product\n",
    "tic = time.perf_counter()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n",
    "# classic general dot product\n",
    "w = np.random.rand(3, len(x1))\n",
    "tic = time.perf_counter()\n",
    "gdot = np.zeros(w.shape[0])\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += w[i,j]*x1[j]\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9d03386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken0.11439999798312783ms.\n",
      "Time taken0.25850001838989556ms.\n",
      "Time taken0.1162000116892159ms.\n",
      "Time taken0.10090001160278916ms.\n"
     ]
    }
   ],
   "source": [
    "# using numpy functions for all these.\n",
    "tic = time.perf_counter()\n",
    "dot = np.dot(x1, x2) # of vectors\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "dot = np.outer(x1, x2)\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "dot = np.multiply(x1, x2) # element wise\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "dot = np.dot(w, x2) # general dot product\n",
    "toc = time.perf_counter()\n",
    "print(\"Time taken\" + str(1000*(toc-tic)) + \"ms.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c5716",
   "metadata": {},
   "source": [
    "np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1efdb1",
   "metadata": {},
   "source": [
    "#### 2.1 Implement the L1 and L2 loss functions.\n",
    "\n",
    "L1: summation of absolute loss between target and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18a03c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    return np.sum(np.abs(y-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "742da605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 =1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print('L1 =' + str(L1(yhat, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e9f0e",
   "metadata": {},
   "source": [
    "L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5bd1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    return np.sum((y-yhat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27e970e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"loss = \" + str(L2(yhat, y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
